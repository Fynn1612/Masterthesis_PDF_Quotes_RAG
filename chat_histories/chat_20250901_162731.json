[
  {
    "question": "What is epistemic uncertainty",
    "answer": "Epistemic uncertainty refers to the lack of knowledge about the process or the finite amount of training data (**uncertainty_slides.pdf**, page 32; **uncertainty_slides.pdf**, page 30; **dietterich-uncertainty-quantification-in-machine-learning-final.pdf**, page 11). It is also known as systematic uncertainty (**Aleatoric and Epistemic Uncertainty in RF.pdf**, page 0).\n\nKey characteristics of epistemic uncertainty:\n*   It detects samples far from the training distribution (**uncertainty_slides.pdf**, page 32; **uncertainty_slides.pdf**, page 33). A model should be more uncertain about its prediction in regions where it was not trained, which can be used to classify inputs as out-of-distribution (**DEEPLY UNCERTAIN COMPARING METHODS OF UNCERTAINTY QUANTIFICATION IN DEEP LEARNING ALGORITHMS.pdf**, page 5; **Deeply uncertain comparing methods of uncertainty quantification in deep learning algorihms.pdf**, page 6).\n*   It disappears or drops given enough data (**uncertainty_slides.pdf**, page 32; **uncertainty_slides.pdf**, page 33; **dietterich-uncertainty-quantification-in-machine-learning-final.pdf**, page 11). As such, it is the reducible part of the total uncertainty (**Aleatoric and Epistemic Uncertainty in RF.pdf**, page 0).\n*   It is associated with the model itself, and the uncertainty quantification method interacts directly with the model (**A Deeper Look into Aleatoric and Epistemic Uncertainty Disentanglement.pdf**, page 5).\n*   High epistemic uncertainty implies that the model's knowledge is limited (**Masterarbeit Detecting_Uncertainty.pdf**, page 24).\n*   Modeling epistemic variance is more beneficial when training data is small (**uncertainty_slides.pdf**, page 112).\n*   It can be detected by training many models and observing where they disagree (**uncertainty_slides.pdf**, page 33).\n*   In a non-stationary world, there is always epistemic uncertainty because enough data is never observed to converge (**dietterich-uncertainty-quantification-in-machine-learning-final.pdf**, page 11).\n*   Epistemic uncertainty will be high if class probabilities strongly vary within the set of plausible hypotheses, meaning there is uncertainty on how to compare these probabilities (**Aleatoric and Epistemic Uncertainty in RF.pdf**, page 4).\n\nThe concepts of aleatoric and epistemic uncertainty are unambiguous when defined within the framework of a model. What is considered aleatoric in one model may be epistemic in another, and depending on design decisions, aleatoric uncertainty can turn into epistemic uncertainty and vice versa (**A Structured Review of Literature on Uncertainty in Machine.pdf**, page 6). Both types of uncertainty play an important role in Machine Learning, where the learner's state of knowledge depends on the amount of data seen so far (**Representation of Quantification of Uncertainty in ML.pdf**, page 11). Epistemic uncertainty is also closely tied to \"model uncertainty\" or \"approximation uncertainty\" (**A Structured Review of Literature on Uncertainty in Machine.pdf**, page 15; **Aleatoric and Epistemic Uncertainty in Machine Learning An Introduction to Concepts and Methods.pdf**, page 6)."
  },
  {
    "question": "explain to me how MC Dropout works",
    "answer": "MC Dropout works by applying the dropout technique during both the training and inference steps of a neural network (**An End-to-End Pipeline for Uncertainty Quantification and Remaining Useful.pdf**, page 6; **Uncertainty-aware prognosis via deep gaussian process.pdf**, page 2).\n\nHere's a breakdown of how it works:\n*   **Mechanism during training:** It follows the same deterministic Deep Neural Network (DNN) training, but forms new sparsely connected DNNs from the original by multiplying every weight with an independent Bernoulli random variable. This means each weight has a certain probability (p) of becoming zero, effectively dropping it out. These Bernoulli random variables are re-sampled for every training sample and every forward pass of the model (**Uncertainty quantification in machine learning for engineering design and health prognostics A tutorial.pdf**, page 18; **Comparison of Uncertainty Quantification with Deep Learning in Time Series Regression.pdf**, page 6).\n*   **Mechanism during inference:** At test time, for a new input, the prediction is repeated with multiple forward passes. Each pass uses a new, randomized sparse DNN resulting from the dropout operation (i.e., a new dropout mask is drawn for each forward pass). This generates multiple outputs for a single input (**Uncertainty quantification in machine learning for engineering design and health prognostics A tutorial.pdf**, page 18; **An End-to-End Pipeline for Uncertainty Quantification and Remaining Useful.pdf**, page 6; **Illustrative Discussion of MC‑Dropout in General Dataset.pdf**, page 3; **Masterarbeit Detecting_Uncertainty.pdf**, page 32).\n*   **Uncertainty Estimation:** With the obtained ensemble of predictions (samples), the model's confidence in its predictions can be estimated (**Uncertainty quantification in machine learning for engineering design and health prognostics A tutorial.pdf**, page 18; **Dropout as a Bayesian Approximation Appendix.pdf**, page 12). Mutual information, for example, is used to express predictive uncertainty and specifically captures the epistemic uncertainty in the model (**Illustrative Discussion of MC‑Dropout in General Dataset.pdf**, page 3).\n*   **Bayesian Approximation:** MC Dropout has been shown to approximate the posterior predictive distribution under a particular Bayesian setup and is considered equivalent to performing approximate Variational Inference (VI) (**Uncertainty quantification in machine learning for engineering design and health prognostics A tutorial.pdf**, page 18; **Bayesian neural networks for uncertainty quantification in.pdf**, page 2; **An End-to-End Pipeline for Uncertainty Quantification and Remaining Useful.pdf**, page 6).\n*   **Advantages:** MC Dropout is simple, scalable, and computationally efficient compared to other Bayesian deep learning approaches. It is straightforward to implement, requiring only minor modifications to an existing DNN setup, and its training procedure is similar to ordinary non-Bayesian DNN training (**An End-to-End Pipeline for Uncertainty Quantification and Remaining Useful.pdf**, page 6; **Uncertainty quantification in machine learning for engineering design and health prognostics A tutorial.pdf**, page 18)."
  },
  {
    "question": "in this context explain heteroscedasti and homoscedastic implementation",
    "answer": "Here's an explanation of homoscedastic and heteroscedastic implementation based on the provided context:\n\n**Homoscedastic uncertainty implementation:**\n*   For regression problems, if using a Laplace Likelihood, the implementation involves minimizing the distance between model predictions and the training data (**uncertainty_slides.pdf**, page 78; **uncertainty_slides.pdf**, page 79).\n\n**Heteroscedastic uncertainty implementation:**\n*   This approach models uncertainty as a function of the input (**uncertainty_slides.pdf**, page 85; **uncertainty_slides.pdf**, page 87).\n*   The learned loss attenuation property of heteroscedastic Neural Networks (NNs) in regression is a desirable effect for classification models, and the ideas can be extended from regression heteroscedastic NNs to classification heteroscedastic NNs (**What Uncertainties Do We Need in Bayesian Deep Learning for Copmuter Vision.pdf**, page 4). Technically, any classification task has input-dependent uncertainty (**What Uncertainties Do We Need in Bayesian Deep Learning for Copmuter Vision.pdf**, page 4)."
  }
]